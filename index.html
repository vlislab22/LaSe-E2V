<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>LaSe-E2V</title>
    <meta name="author" content="KHao">
    <meta name="description" content="Project page of LaSe-E2V">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>
    <style>
        .center-image {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

        body {
            margin: 0;
            text-align: center;
        }

        .container {
            margin: 20px auto;
            padding: 15px;
            max-width: 1200px;
        }

        .video-container {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 10px;
        }

        .video-item {
            flex: 1 1 calc(33.333% - 10px);
            box-sizing: border-box;
            margin: 5px;
        }

        @media (max-width: 992px) {
            .video-item {
                flex: 1 1 calc(50% - 10px);
            }
        }

        @media (max-width: 768px) {
            .video-item {
                flex: 1 1 100%;
            }
        }
    </style>
  </head>
	
  <body>

    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                LaSe-E2V: Towards Language-guided Semantic-Aware Event-to-Video Reconstruction<br /> 
                <small>
                    Arkiv
                </small>
            </h1>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <img src="./images/khao.jpg" height="80px"><br>
                        <a href="https://khao123.github.io/" >
                            Xu Zheng
                        </a>
                        <br /> AI Thrust, HKUST(GZ)
                        <br /> &nbsp &nbsp
                    </li>
                    
                    <li>
                        <img src="./images/hangyu.png" height="80px"><br>
                        <a href="https://github.com/cyjdlhy" >
                            Hangyu Li
                        </a>
                        <br /> AI Thrust, HKUST(GZ)
                        <br /> &nbsp &nbsp
                    </li>
                    
                    <li>
                        <img src="./images/zjz.jpg" height="80px"><br>
                        <a href="https://jiazhou-garland.github.io/" >
                            Jiazhou Zhou
                        </a>
                        <br /> AI Thrust, HKUST(GZ)
                        <br /> &nbsp &nbsp
                    </li>
                    
                    <li>
                        <img src="./images/Addision.png" height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">
                            Addison Lin Wang
                        </a>
                        <br /> AI & CMA Thrust, HKUST(GZ) 
                        <br/> Dept. of CSE, HKUST 
                    </li>
                </ul>
            </div>
        </div>

        <!-- ##### Elements #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="">
                            <img src="./images/arxiv.png" height="100px"><br>
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    
                    <li> 
                        <a href="https://github.com/KHao123/LaSe-E2V">
                            <img src="./images/github_icon.jpg" height="100px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
                    
                    <li>
                        <a href="https://vlislab22.github.io/vlislab/">
                            <img src="./images/lab_logo.png" height="100px"><br>
                            <h4><strong>Vlislab</strong></h4>
                        </a>
                    </li>                       
                </ul>
            </div>
        </div>

        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Event cameras harness advantages such as low latency, high temporal resolution, and high dynamic range (HDR), compared to standard cameras. 
Due to the distinct imaging paradigm shift, a dominant line of research focuses on event-to-video (E2V) reconstruction to bridge event-based and standard computer vision. 
However, this task remains challenging due to its inherently ill-posed nature: event cameras only detect the edge and motion information locally. 
Consequently, the reconstructed videos are often plagued by artifacts and regional blur, primarily caused by the ambiguous semantics of event data. 
In this paper, we find language naturally conveys abundant semantic information, rendering it stunningly superior in ensuring semantic consistency for E2V reconstruction. 
Accordingly, we propose a novel framework, called LaSe-E2V, that can achieve semantic-aware high-quality E2V reconstruction from a language-guided perspective, buttressed by the text-conditional diffusion models.
However, due to diffusion models' inherent diversity and randomness, it is hardly possible to directly apply them to achieve spatial and temporal consistency for E2V reconstruction.
Thus, we first propose an Event-guided Spatiotemporal Attention (ESA) module to condition the event data to the denoising pipeline effectively. 
We then introduce an event-aware mask loss to ensure temporal coherence and a noise initialization strategy to enhance spatial consistency.
Given the absence of event-text-video paired data, we aggregate existing E2V datasets and generate textual descriptions using the tagging models for training and evaluation. 
Extensive experiments on three datasets covering diverse challenging scenarios, e.g, fast motion, low light demonstrate the superiority of our method.
                </p>
            </div>
        </div>

	<!-- ##### Results #####-->
 <!--
	    <div class="row">
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Qualitative Results.
          </h3>   
    </div>   
         <div class="col-md-8 col-md-offset-2">
            <video width="750"  controls >
                <source src="./video/general.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>  
	<p class="text-justify">
While the previous approaches suffer from low contrast, blur, and extensive artifacts, LaSe-E2V obtains clear edges with high contrast and preserves the semantic details of the objects.
	</p>
    </div>          
      </div>

<div class="row">
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Qualitative results of fast-motion condition from HS-ERGB dataset.
          </h3>   
    </div>   
         <div class="col-md-8 col-md-offset-2">
            <video width="750"  controls >
                <source src="./video/fast-motion.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>   
    </div>          
      </div>

<div class="row">
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Qualitative results in low light condition from MVSEC dataset.
          </h3>   
    </div>   
         <div class="col-md-8 col-md-offset-2">
            <video width="750"  controls >
                <source src="./video/low-light.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>   
	<p class="text-justify">
	LaSe-E2V performs better to preserve the HDR characteristic of event cameras with higher contrast.
	</p>
    </div>          
      </div> -->

    <div class="video-container">
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_bike_bay_hdr.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_indoor_flying2.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_outdoor_day2.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_poster_6dof.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_poster_pillar.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/comparison_slow_and_fast_desk.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/fast_1.mp4" type="video/mp4">
            </video>
        </div>
        <div class="video-item">
            <video width="100%" height="auto" controls>
                <source src="./video/fast_2.mp4" type="video/mp4">
            </video>
        </div>
    </div>
	    
    <!-- ##### Results #####-->

     <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
               Comparison with the Previous Baseline
            </h3>
		    <p class="text-justify">
		        The baseline method (HyperE2VID) solely relies on event data, leading to ambiguity in local structures. 
		        In contrast, our approach integrates language descriptions to enrich the semantic information and ensure the video remains coherent with the event stream.
		    </p>
            <img src="./images/e2v_overview.png" class="img-responsive" alt="vis_res" class="center-image">
        </div>
    </div>
	    
    <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
               Overview Framework.
            </h3>
            <img src="./images/framework.png" class="img-responsive" alt="vis_res" class="center-image">
        </div>
    </div>
	  
   <!-- ##### BibTex ##### -->
   <!--
   <hr>
   <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>BibTeX</h3>
            <div class="row align-items-center">
                <div class="col py-3">
                    <pre class="border">
@article{ExACT,
  title={ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More},
  author={Zhou,Jiazhou, Zheng,Xu, Lyu,Yuanhuiyi and Wang,Lin},
  journal={The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}}
                    </pre>
                </div>
            </div>
        </div>
    </div>
    -->
</body>
</html>
